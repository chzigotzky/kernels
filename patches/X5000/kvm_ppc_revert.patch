diff -rupN a/arch/powerpc/kvm/e500_mmu_host.c b/arch/powerpc/kvm/e500_mmu_host.c
--- a/arch/powerpc/kvm/e500_mmu_host.c	2024-12-22 22:22:21.000000000 +0100
+++ b/arch/powerpc/kvm/e500_mmu_host.c	2024-12-28 12:20:18.588599608 +0100
@@ -242,7 +242,7 @@ static inline int tlbe_is_writable(struc
 	return tlbe->mas7_3 & (MAS3_SW|MAS3_UW);
 }
 
-static inline bool kvmppc_e500_ref_setup(struct tlbe_ref *ref,
+static inline void kvmppc_e500_ref_setup(struct tlbe_ref *ref,
 					 struct kvm_book3e_206_tlb_entry *gtlbe,
 					 kvm_pfn_t pfn, unsigned int wimg)
 {
@@ -252,7 +252,6 @@ static inline bool kvmppc_e500_ref_setup
 	/* Use guest supplied MAS2_G and MAS2_E */
 	ref->flags |= (gtlbe->mas2 & MAS2_ATTRIB_MASK) | wimg;
 
-	return tlbe_is_writable(gtlbe);
 }
 
 static inline void kvmppc_e500_ref_release(struct tlbe_ref *ref)
@@ -322,7 +321,6 @@ static inline int kvmppc_e500_shadow_map
 {
 	struct kvm_memory_slot *slot;
 	unsigned long pfn = 0; /* silence GCC warning */
-	struct page *page = NULL;
 	unsigned long hva;
 	int pfnmap = 0;
 	int tsize = BOOK3E_PAGESZ_4K;
@@ -334,7 +332,6 @@ static inline int kvmppc_e500_shadow_map
 	unsigned int wimg = 0;
 	pgd_t *pgdir;
 	unsigned long flags;
-	bool writable = false;
 
 	/* used to check for invalidations in progress */
 	mmu_seq = kvm->mmu_invalidate_seq;
@@ -444,7 +441,7 @@ static inline int kvmppc_e500_shadow_map
 
 	if (likely(!pfnmap)) {
 		tsize_pages = 1UL << (tsize + 10 - PAGE_SHIFT);
-		pfn = __kvm_faultin_pfn(slot, gfn, FOLL_WRITE, NULL, &page);
+		pfn = gfn_to_pfn_memslot(slot, gfn);
 		if (is_error_noslot_pfn(pfn)) {
 			if (printk_ratelimit())
 				pr_err("%s: real page not found for gfn %lx\n",
@@ -488,7 +485,7 @@ static inline int kvmppc_e500_shadow_map
 			goto out;
 		}
 	}
-	writable = kvmppc_e500_ref_setup(ref, gtlbe, pfn, wimg);
+	kvmppc_e500_ref_setup(ref, gtlbe, pfn, wimg);
 
 	kvmppc_e500_setup_stlbe(&vcpu_e500->vcpu, gtlbe, tsize,
 				ref, gvaddr, stlbe);
@@ -497,8 +494,11 @@ static inline int kvmppc_e500_shadow_map
 	kvmppc_mmu_flush_icache(pfn);
 
 out:
-	kvm_release_faultin_page(kvm, page, !!ret, writable);
 	spin_unlock(&kvm->mmu_lock);
+
+	/* Drop refcount on page, so that mmu notifiers can clear it */
+	kvm_release_pfn_clean(pfn);
+
 	return ret;
 }
 
diff -rupN a/include/linux/kvm_host.h b/include/linux/kvm_host.h
--- a/include/linux/kvm_host.h	2024-12-22 22:22:21.000000000 +0100
+++ b/include/linux/kvm_host.h	2024-12-28 12:18:30.583120006 +0100
@@ -97,7 +97,6 @@
 #define KVM_PFN_ERR_HWPOISON	(KVM_PFN_ERR_MASK + 1)
 #define KVM_PFN_ERR_RO_FAULT	(KVM_PFN_ERR_MASK + 2)
 #define KVM_PFN_ERR_SIGPENDING	(KVM_PFN_ERR_MASK + 3)
-#define KVM_PFN_ERR_NEEDS_IO	(KVM_PFN_ERR_MASK + 4)
 
 /*
  * error pfns indicate that the gfn is in slot but faild to
@@ -154,6 +153,13 @@ static inline bool kvm_is_error_gpa(gpa_
 	return gpa == INVALID_GPA;
 }
 
+#define KVM_ERR_PTR_BAD_PAGE	(ERR_PTR(-ENOENT))
+
+static inline bool is_error_page(struct page *page)
+{
+	return IS_ERR(page);
+}
+
 #define KVM_REQUEST_MASK           GENMASK(7,0)
 #define KVM_REQUEST_NO_WAKEUP      BIT(8)
 #define KVM_REQUEST_WAIT           BIT(9)
@@ -213,7 +219,6 @@ enum kvm_bus {
 	KVM_PIO_BUS,
 	KVM_VIRTIO_CCW_NOTIFY_BUS,
 	KVM_FAST_MMIO_BUS,
-	KVM_IOCSR_BUS,
 	KVM_NR_BUSES
 };
 
@@ -274,19 +279,21 @@ enum {
 	READING_SHADOW_PAGE_TABLES,
 };
 
+#define KVM_UNMAPPED_PAGE	((void *) 0x500 + POISON_POINTER_DELTA)
+
 struct kvm_host_map {
 	/*
 	 * Only valid if the 'pfn' is managed by the host kernel (i.e. There is
 	 * a 'struct page' for it. When using mem= kernel parameter some memory
 	 * can be used as guest memory but they are not managed by host
 	 * kernel).
+	 * If 'pfn' is not managed by the host kernel, this field is
+	 * initialized to KVM_UNMAPPED_PAGE.
 	 */
-	struct page *pinned_page;
 	struct page *page;
 	void *hva;
 	kvm_pfn_t pfn;
 	kvm_pfn_t gfn;
-	bool writable;
 };
 
 /*
@@ -335,8 +342,7 @@ struct kvm_vcpu {
 #ifndef __KVM_HAVE_ARCH_WQP
 	struct rcuwait wait;
 #endif
-	struct pid *pid;
-	rwlock_t pid_lock;
+	struct pid __rcu *pid;
 	int sigset_active;
 	sigset_t sigset;
 	unsigned int halt_poll_ns;
@@ -1170,10 +1176,6 @@ static inline bool kvm_memslot_iter_is_v
 	     kvm_memslot_iter_is_valid(iter, end);			\
 	     kvm_memslot_iter_next(iter))
 
-struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
-struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
-struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
-
 /*
  * KVM_SET_USER_MEMORY_REGION ioctl allows the following operations:
  * - create a new memory slot
@@ -1212,70 +1214,33 @@ void kvm_arch_flush_shadow_all(struct kv
 void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
 				   struct kvm_memory_slot *slot);
 
-int kvm_prefetch_pages(struct kvm_memory_slot *slot, gfn_t gfn,
-		       struct page **pages, int nr_pages);
-
-struct page *__gfn_to_page(struct kvm *kvm, gfn_t gfn, bool write);
-static inline struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)
-{
-	return __gfn_to_page(kvm, gfn, true);
-}
+int gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,
+			    struct page **pages, int nr_pages);
 
+struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);
 unsigned long gfn_to_hva_prot(struct kvm *kvm, gfn_t gfn, bool *writable);
 unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
 unsigned long gfn_to_hva_memslot_prot(struct kvm_memory_slot *slot, gfn_t gfn,
 				      bool *writable);
-
-static inline void kvm_release_page_unused(struct page *page)
-{
-	if (!page)
-		return;
-
-	put_page(page);
-}
-
 void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 
-static inline void kvm_release_faultin_page(struct kvm *kvm, struct page *page,
-					    bool unused, bool dirty)
-{
-	lockdep_assert_once(lockdep_is_held(&kvm->mmu_lock) || unused);
-
-	if (!page)
-		return;
-
-	/*
-	 * If the page that KVM got from the *primary MMU* is writable, and KVM
-	 * installed or reused a SPTE, mark the page/folio dirty.  Note, this
-	 * may mark a folio dirty even if KVM created a read-only SPTE, e.g. if
-	 * the GFN is write-protected.  Folios can't be safely marked dirty
-	 * outside of mmu_lock as doing so could race with writeback on the
-	 * folio.  As a result, KVM can't mark folios dirty in the fast page
-	 * fault handler, and so KVM must (somewhat) speculatively mark the
-	 * folio dirty if KVM could locklessly make the SPTE writable.
-	 */
-	if (unused)
-		kvm_release_page_unused(page);
-	else if (dirty)
-		kvm_release_page_dirty(page);
-	else
-		kvm_release_page_clean(page);
-}
-
-kvm_pfn_t __kvm_faultin_pfn(const struct kvm_memory_slot *slot, gfn_t gfn,
-			    unsigned int foll, bool *writable,
-			    struct page **refcounted_page);
-
-static inline kvm_pfn_t kvm_faultin_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,
-					bool write, bool *writable,
-					struct page **refcounted_page)
-{
-	return __kvm_faultin_pfn(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn,
-				 write ? FOLL_WRITE : 0, writable, refcounted_page);
-}
+kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
+		      bool *writable);
+kvm_pfn_t gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn_memslot_atomic(const struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,
+			       bool atomic, bool interruptible, bool *async,
+			       bool write_fault, bool *writable, hva_t *hva);
+
+void kvm_release_pfn_clean(kvm_pfn_t pfn);
+void kvm_release_pfn_dirty(kvm_pfn_t pfn);
+void kvm_set_pfn_dirty(kvm_pfn_t pfn);
+void kvm_set_pfn_accessed(kvm_pfn_t pfn);
 
+void kvm_release_pfn(kvm_pfn_t pfn, bool dirty);
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
 int kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);
@@ -1339,28 +1304,17 @@ int kvm_gfn_to_hva_cache_init(struct kvm
 })
 
 int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);
+struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
 bool kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
 bool kvm_vcpu_is_visible_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_host_page_size(struct kvm_vcpu *vcpu, gfn_t gfn);
 void mark_page_dirty_in_slot(struct kvm *kvm, const struct kvm_memory_slot *memslot, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
-int __kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map,
-		   bool writable);
-void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map);
-
-static inline int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa,
-			       struct kvm_host_map *map)
-{
-	return __kvm_vcpu_map(vcpu, gpa, map, true);
-}
-
-static inline int kvm_vcpu_map_readonly(struct kvm_vcpu *vcpu, gpa_t gpa,
-					struct kvm_host_map *map)
-{
-	return __kvm_vcpu_map(vcpu, gpa, map, false);
-}
-
+struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
+struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
+int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map);
+void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty);
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
 int kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data, int offset,
@@ -1732,6 +1686,9 @@ void kvm_arch_sync_events(struct kvm *kv
 
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 
+struct page *kvm_pfn_to_refcounted_page(kvm_pfn_t pfn);
+bool kvm_is_zone_device_page(struct page *page);
+
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;
 	unsigned gsi;
@@ -2498,13 +2455,11 @@ static inline bool kvm_mem_is_private(st
 
 #ifdef CONFIG_KVM_PRIVATE_MEM
 int kvm_gmem_get_pfn(struct kvm *kvm, struct kvm_memory_slot *slot,
-		     gfn_t gfn, kvm_pfn_t *pfn, struct page **page,
-		     int *max_order);
+		     gfn_t gfn, kvm_pfn_t *pfn, int *max_order);
 #else
 static inline int kvm_gmem_get_pfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
-				   kvm_pfn_t *pfn, struct page **page,
-				   int *max_order)
+				   kvm_pfn_t *pfn, int *max_order)
 {
 	KVM_BUG_ON(1, kvm);
 	return -EIO;
diff -rupN a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
--- a/virt/kvm/kvm_main.c	2024-12-22 22:22:21.000000000 +0100
+++ b/virt/kvm/kvm_main.c	2024-12-28 12:18:30.585119978 +0100
@@ -95,13 +95,6 @@ module_param(halt_poll_ns_shrink, uint,
 EXPORT_SYMBOL_GPL(halt_poll_ns_shrink);
 
 /*
- * Allow direct access (from KVM or the CPU) without MMU notifier protection
- * to unpinned pages.
- */
-static bool allow_unsafe_mappings;
-module_param(allow_unsafe_mappings, bool, 0444);
-
-/*
  * Ordering of locks:
  *
  *	kvm->lock --> kvm->slots_lock --> kvm->irq_lock
@@ -160,6 +153,52 @@ __weak void kvm_arch_guest_memory_reclai
 {
 }
 
+bool kvm_is_zone_device_page(struct page *page)
+{
+	/*
+	 * The metadata used by is_zone_device_page() to determine whether or
+	 * not a page is ZONE_DEVICE is guaranteed to be valid if and only if
+	 * the device has been pinned, e.g. by get_user_pages().  WARN if the
+	 * page_count() is zero to help detect bad usage of this helper.
+	 */
+	if (WARN_ON_ONCE(!page_count(page)))
+		return false;
+
+	return is_zone_device_page(page);
+}
+
+/*
+ * Returns a 'struct page' if the pfn is "valid" and backed by a refcounted
+ * page, NULL otherwise.  Note, the list of refcounted PG_reserved page types
+ * is likely incomplete, it has been compiled purely through people wanting to
+ * back guest with a certain type of memory and encountering issues.
+ */
+struct page *kvm_pfn_to_refcounted_page(kvm_pfn_t pfn)
+{
+	struct page *page;
+
+	if (!pfn_valid(pfn))
+		return NULL;
+
+	page = pfn_to_page(pfn);
+	if (!PageReserved(page))
+		return page;
+
+	/* The ZERO_PAGE(s) is marked PG_reserved, but is refcounted. */
+	if (is_zero_pfn(pfn))
+		return page;
+
+	/*
+	 * ZONE_DEVICE pages currently set PG_reserved, but from a refcounting
+	 * perspective they are "normal" pages, albeit with slightly different
+	 * usage rules.
+	 */
+	if (kvm_is_zone_device_page(page))
+		return page;
+
+	return NULL;
+}
+
 /*
  * Switches to specified vcpu, until a matching vcpu_put()
  */
@@ -447,7 +486,6 @@ static void kvm_vcpu_init(struct kvm_vcp
 	vcpu->kvm = kvm;
 	vcpu->vcpu_id = id;
 	vcpu->pid = NULL;
-	rwlock_init(&vcpu->pid_lock);
 #ifndef __KVM_HAVE_ARCH_WQP
 	rcuwait_init(&vcpu->wait);
 #endif
@@ -475,7 +513,7 @@ static void kvm_vcpu_destroy(struct kvm_
 	 * the vcpu->pid pointer, and at destruction time all file descriptors
 	 * are already gone.
 	 */
-	put_pid(vcpu->pid);
+	put_pid(rcu_dereference_protected(vcpu->pid, 1));
 
 	free_page((unsigned long)vcpu->run);
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
@@ -631,8 +669,7 @@ mmu_unlock:
 static __always_inline int kvm_handle_hva_range(struct mmu_notifier *mn,
 						unsigned long start,
 						unsigned long end,
-						gfn_handler_t handler,
-						bool flush_on_ret)
+						gfn_handler_t handler)
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	const struct kvm_mmu_notifier_range range = {
@@ -640,7 +677,7 @@ static __always_inline int kvm_handle_hv
 		.end		= end,
 		.handler	= handler,
 		.on_lock	= (void *)kvm_null_fn,
-		.flush_on_ret	= flush_on_ret,
+		.flush_on_ret	= true,
 		.may_block	= false,
 	};
 
@@ -652,7 +689,17 @@ static __always_inline int kvm_handle_hv
 							 unsigned long end,
 							 gfn_handler_t handler)
 {
-	return kvm_handle_hva_range(mn, start, end, handler, false);
+	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+	const struct kvm_mmu_notifier_range range = {
+		.start		= start,
+		.end		= end,
+		.handler	= handler,
+		.on_lock	= (void *)kvm_null_fn,
+		.flush_on_ret	= false,
+		.may_block	= false,
+	};
+
+	return __kvm_handle_hva_range(kvm, &range).ret;
 }
 
 void kvm_mmu_invalidate_begin(struct kvm *kvm)
@@ -817,8 +864,7 @@ static int kvm_mmu_notifier_clear_flush_
 {
 	trace_kvm_age_hva(start, end);
 
-	return kvm_handle_hva_range(mn, start, end, kvm_age_gfn,
-				    !IS_ENABLED(CONFIG_KVM_ELIDE_TLB_FLUSH_IF_YOUNG));
+	return kvm_handle_hva_range(mn, start, end, kvm_age_gfn);
 }
 
 static int kvm_mmu_notifier_clear_young(struct mmu_notifier *mn,
@@ -2700,93 +2746,37 @@ unsigned long kvm_vcpu_gfn_to_hva_prot(s
 	return gfn_to_hva_memslot_prot(slot, gfn, writable);
 }
 
-static bool kvm_is_ad_tracked_page(struct page *page)
-{
-	/*
-	 * Per page-flags.h, pages tagged PG_reserved "should in general not be
-	 * touched (e.g. set dirty) except by its owner".
-	 */
-	return !PageReserved(page);
-}
-
-static void kvm_set_page_dirty(struct page *page)
-{
-	if (kvm_is_ad_tracked_page(page))
-		SetPageDirty(page);
-}
-
-static void kvm_set_page_accessed(struct page *page)
-{
-	if (kvm_is_ad_tracked_page(page))
-		mark_page_accessed(page);
-}
-
-void kvm_release_page_clean(struct page *page)
-{
-	if (!page)
-		return;
-
-	kvm_set_page_accessed(page);
-	put_page(page);
-}
-EXPORT_SYMBOL_GPL(kvm_release_page_clean);
-
-void kvm_release_page_dirty(struct page *page)
-{
-	if (!page)
-		return;
-
-	kvm_set_page_dirty(page);
-	kvm_release_page_clean(page);
-}
-EXPORT_SYMBOL_GPL(kvm_release_page_dirty);
-
-static kvm_pfn_t kvm_resolve_pfn(struct kvm_follow_pfn *kfp, struct page *page,
-				 struct follow_pfnmap_args *map, bool writable)
+static inline int check_user_page_hwpoison(unsigned long addr)
 {
-	kvm_pfn_t pfn;
-
-	WARN_ON_ONCE(!!page == !!map);
+	int rc, flags = FOLL_HWPOISON | FOLL_WRITE;
 
-	if (kfp->map_writable)
-		*kfp->map_writable = writable;
-
-	if (map)
-		pfn = map->pfn;
-	else
-		pfn = page_to_pfn(page);
-
-	*kfp->refcounted_page = page;
-
-	return pfn;
+	rc = get_user_pages(addr, 1, flags, NULL);
+	return rc == -EHWPOISON;
 }
 
 /*
  * The fast path to get the writable pfn which will be stored in @pfn,
- * true indicates success, otherwise false is returned.
+ * true indicates success, otherwise false is returned.  It's also the
+ * only part that runs if we can in atomic context.
  */
-static bool hva_to_pfn_fast(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
+static bool hva_to_pfn_fast(unsigned long addr, bool write_fault,
+			    bool *writable, kvm_pfn_t *pfn)
 {
-	struct page *page;
-	bool r;
+	struct page *page[1];
 
 	/*
-	 * Try the fast-only path when the caller wants to pin/get the page for
-	 * writing.  If the caller only wants to read the page, KVM must go
-	 * down the full, slow path in order to avoid racing an operation that
-	 * breaks Copy-on-Write (CoW), e.g. so that KVM doesn't end up pointing
-	 * at the old, read-only page while mm/ points at a new, writable page.
+	 * Fast pin a writable pfn only if it is a write fault request
+	 * or the caller allows to map a writable pfn for a read fault
+	 * request.
 	 */
-	if (!((kfp->flags & FOLL_WRITE) || kfp->map_writable))
+	if (!(write_fault || writable))
 		return false;
 
-	if (kfp->pin)
-		r = pin_user_pages_fast(kfp->hva, 1, FOLL_WRITE, &page) == 1;
-	else
-		r = get_user_page_fast_only(kfp->hva, FOLL_WRITE, &page);
+	if (get_user_page_fast_only(addr, FOLL_WRITE, page)) {
+		*pfn = page_to_pfn(page[0]);
 
-	if (r) {
-		*pfn = kvm_resolve_pfn(kfp, page, NULL, true);
+		if (writable)
+			*writable = true;
 		return true;
 	}
 
@@ -2797,7 +2787,8 @@ static bool hva_to_pfn_fast(struct kvm_f
  * The slow path to get the pfn of the specified host virtual address,
  * 1 indicates success, -errno is returned if error is detected.
  */
-static int hva_to_pfn_slow(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
+static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
+			   bool interruptible, bool *writable, kvm_pfn_t *pfn)
 {
 	/*
 	 * When a VCPU accesses a page that is not mapped into the secondary
@@ -2810,35 +2801,37 @@ static int hva_to_pfn_slow(struct kvm_fo
 	 * Note that get_user_page_fast_only() and FOLL_WRITE for now
 	 * implicitly honor NUMA hinting faults and don't need this flag.
 	 */
-	unsigned int flags = FOLL_HWPOISON | FOLL_HONOR_NUMA_FAULT | kfp->flags;
-	struct page *page, *wpage;
+	unsigned int flags = FOLL_HWPOISON | FOLL_HONOR_NUMA_FAULT;
+	struct page *page;
 	int npages;
 
-	if (kfp->pin)
-		npages = pin_user_pages_unlocked(kfp->hva, 1, &page, flags);
-	else
-		npages = get_user_pages_unlocked(kfp->hva, 1, &page, flags);
+	might_sleep();
+
+	if (writable)
+		*writable = write_fault;
+
+	if (write_fault)
+		flags |= FOLL_WRITE;
+	if (async)
+		flags |= FOLL_NOWAIT;
+	if (interruptible)
+		flags |= FOLL_INTERRUPTIBLE;
+
+	npages = get_user_pages_unlocked(addr, 1, &page, flags);
 	if (npages != 1)
 		return npages;
 
-	/*
-	 * Pinning is mutually exclusive with opportunistically mapping a read
-	 * fault as writable, as KVM should never pin pages when mapping memory
-	 * into the guest (pinning is only for direct accesses from KVM).
-	 */
-	if (WARN_ON_ONCE(kfp->map_writable && kfp->pin))
-		goto out;
-
 	/* map read fault as writable if possible */
-	if (!(flags & FOLL_WRITE) && kfp->map_writable &&
-	    get_user_page_fast_only(kfp->hva, FOLL_WRITE, &wpage)) {
-		put_page(page);
-		page = wpage;
-		flags |= FOLL_WRITE;
-	}
+	if (unlikely(!write_fault) && writable) {
+		struct page *wpage;
 
-out:
-	*pfn = kvm_resolve_pfn(kfp, page, NULL, flags & FOLL_WRITE);
+		if (get_user_page_fast_only(addr, FOLL_WRITE, &wpage)) {
+			*writable = true;
+			put_page(page);
+			page = wpage;
+		}
+	}
+	*pfn = page_to_pfn(page);
 	return npages;
 }
 
@@ -2853,21 +2846,24 @@ static bool vma_is_valid(struct vm_area_
 	return true;
 }
 
+static int kvm_try_get_pfn(kvm_pfn_t pfn)
+{
+	struct page *page = kvm_pfn_to_refcounted_page(pfn);
+
+	if (!page)
+		return 1;
+
+	return get_page_unless_zero(page);
+}
+
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
-			       struct kvm_follow_pfn *kfp, kvm_pfn_t *p_pfn)
+			       unsigned long addr, bool write_fault,
+			       bool *writable, kvm_pfn_t *p_pfn)
 {
-	struct follow_pfnmap_args args = { .vma = vma, .address = kfp->hva };
-	bool write_fault = kfp->flags & FOLL_WRITE;
+	struct follow_pfnmap_args args = { .vma = vma, .address = addr };
+	kvm_pfn_t pfn;
 	int r;
 
-	/*
-	 * Remapped memory cannot be pinned in any meaningful sense.  Bail if
-	 * the caller wants to pin the page, i.e. access the page outside of
-	 * MMU notifier protection, and unsafe umappings are disallowed.
-	 */
-	if (kfp->pin && !allow_unsafe_mappings)
-		return -EINVAL;
-
 	r = follow_pfnmap_start(&args);
 	if (r) {
 		/*
@@ -2875,7 +2871,7 @@ static int hva_to_pfn_remapped(struct vm
 		 * not call the fault handler, so do it here.
 		 */
 		bool unlocked = false;
-		r = fixup_user_fault(current->mm, kfp->hva,
+		r = fixup_user_fault(current->mm, addr,
 				     (write_fault ? FAULT_FLAG_WRITE : 0),
 				     &unlocked);
 		if (unlocked)
@@ -2889,104 +2885,164 @@ static int hva_to_pfn_remapped(struct vm
 	}
 
 	if (write_fault && !args.writable) {
-		*p_pfn = KVM_PFN_ERR_RO_FAULT;
+		pfn = KVM_PFN_ERR_RO_FAULT;
 		goto out;
 	}
 
-	*p_pfn = kvm_resolve_pfn(kfp, NULL, &args, args.writable);
+	if (writable)
+		*writable = args.writable;
+	pfn = args.pfn;
+
+	/*
+	 * Get a reference here because callers of *hva_to_pfn* and
+	 * *gfn_to_pfn* ultimately call kvm_release_pfn_clean on the
+	 * returned pfn.  This is only needed if the VMA has VM_MIXEDMAP
+	 * set, but the kvm_try_get_pfn/kvm_release_pfn_clean pair will
+	 * simply do nothing for reserved pfns.
+	 *
+	 * Whoever called remap_pfn_range is also going to call e.g.
+	 * unmap_mapping_range before the underlying pages are freed,
+	 * causing a call to our MMU notifier.
+	 *
+	 * Certain IO or PFNMAP mappings can be backed with valid
+	 * struct pages, but be allocated without refcounting e.g.,
+	 * tail pages of non-compound higher order allocations, which
+	 * would then underflow the refcount when the caller does the
+	 * required put_page. Don't allow those pages here.
+	 */
+	if (!kvm_try_get_pfn(pfn))
+		r = -EFAULT;
 out:
 	follow_pfnmap_end(&args);
+	*p_pfn = pfn;
+
 	return r;
 }
 
-kvm_pfn_t hva_to_pfn(struct kvm_follow_pfn *kfp)
+/*
+ * Pin guest page in memory and return its pfn.
+ * @addr: host virtual address which maps memory to the guest
+ * @atomic: whether this function is forbidden from sleeping
+ * @interruptible: whether the process can be interrupted by non-fatal signals
+ * @async: whether this function need to wait IO complete if the
+ *         host page is not in the memory
+ * @write_fault: whether we should get a writable host page
+ * @writable: whether it allows to map a writable host page for !@write_fault
+ *
+ * The function will map a writable host page for these two cases:
+ * 1): @write_fault = true
+ * 2): @write_fault = false && @writable, @writable will tell the caller
+ *     whether the mapping is writable.
+ */
+kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool interruptible,
+		     bool *async, bool write_fault, bool *writable)
 {
 	struct vm_area_struct *vma;
 	kvm_pfn_t pfn;
 	int npages, r;
 
-	might_sleep();
-
-	if (WARN_ON_ONCE(!kfp->refcounted_page))
-		return KVM_PFN_ERR_FAULT;
+	/* we can do it either atomically or asynchronously, not both */
+	BUG_ON(atomic && async);
 
-	if (hva_to_pfn_fast(kfp, &pfn))
+	if (hva_to_pfn_fast(addr, write_fault, writable, &pfn))
 		return pfn;
 
-	npages = hva_to_pfn_slow(kfp, &pfn);
+	if (atomic)
+		return KVM_PFN_ERR_FAULT;
+
+	npages = hva_to_pfn_slow(addr, async, write_fault, interruptible,
+				 writable, &pfn);
 	if (npages == 1)
 		return pfn;
-	if (npages == -EINTR || npages == -EAGAIN)
+	if (npages == -EINTR)
 		return KVM_PFN_ERR_SIGPENDING;
-	if (npages == -EHWPOISON)
-		return KVM_PFN_ERR_HWPOISON;
 
 	mmap_read_lock(current->mm);
+	if (npages == -EHWPOISON ||
+	      (!async && check_user_page_hwpoison(addr))) {
+		pfn = KVM_PFN_ERR_HWPOISON;
+		goto exit;
+	}
+
 retry:
-	vma = vma_lookup(current->mm, kfp->hva);
+	vma = vma_lookup(current->mm, addr);
 
 	if (vma == NULL)
 		pfn = KVM_PFN_ERR_FAULT;
 	else if (vma->vm_flags & (VM_IO | VM_PFNMAP)) {
-		r = hva_to_pfn_remapped(vma, kfp, &pfn);
+		r = hva_to_pfn_remapped(vma, addr, write_fault, writable, &pfn);
 		if (r == -EAGAIN)
 			goto retry;
 		if (r < 0)
 			pfn = KVM_PFN_ERR_FAULT;
 	} else {
-		if ((kfp->flags & FOLL_NOWAIT) &&
-		    vma_is_valid(vma, kfp->flags & FOLL_WRITE))
-			pfn = KVM_PFN_ERR_NEEDS_IO;
-		else
-			pfn = KVM_PFN_ERR_FAULT;
+		if (async && vma_is_valid(vma, write_fault))
+			*async = true;
+		pfn = KVM_PFN_ERR_FAULT;
 	}
+exit:
 	mmap_read_unlock(current->mm);
 	return pfn;
 }
 
-static kvm_pfn_t kvm_follow_pfn(struct kvm_follow_pfn *kfp)
+kvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,
+			       bool atomic, bool interruptible, bool *async,
+			       bool write_fault, bool *writable, hva_t *hva)
 {
-	kfp->hva = __gfn_to_hva_many(kfp->slot, kfp->gfn, NULL,
-				     kfp->flags & FOLL_WRITE);
+	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
+
+	if (hva)
+		*hva = addr;
 
-	if (kfp->hva == KVM_HVA_ERR_RO_BAD)
-		return KVM_PFN_ERR_RO_FAULT;
+	if (kvm_is_error_hva(addr)) {
+		if (writable)
+			*writable = false;
 
-	if (kvm_is_error_hva(kfp->hva))
-		return KVM_PFN_NOSLOT;
+		return addr == KVM_HVA_ERR_RO_BAD ? KVM_PFN_ERR_RO_FAULT :
+						    KVM_PFN_NOSLOT;
+	}
 
-	if (memslot_is_readonly(kfp->slot) && kfp->map_writable) {
-		*kfp->map_writable = false;
-		kfp->map_writable = NULL;
+	/* Do not map writable pfn in the readonly memslot. */
+	if (writable && memslot_is_readonly(slot)) {
+		*writable = false;
+		writable = NULL;
 	}
 
-	return hva_to_pfn(kfp);
+	return hva_to_pfn(addr, atomic, interruptible, async, write_fault,
+			  writable);
 }
+EXPORT_SYMBOL_GPL(__gfn_to_pfn_memslot);
 
-kvm_pfn_t __kvm_faultin_pfn(const struct kvm_memory_slot *slot, gfn_t gfn,
-			    unsigned int foll, bool *writable,
-			    struct page **refcounted_page)
+kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
+		      bool *writable)
 {
-	struct kvm_follow_pfn kfp = {
-		.slot = slot,
-		.gfn = gfn,
-		.flags = foll,
-		.map_writable = writable,
-		.refcounted_page = refcounted_page,
-	};
+	return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, false,
+				    NULL, write_fault, writable, NULL);
+}
+EXPORT_SYMBOL_GPL(gfn_to_pfn_prot);
 
-	if (WARN_ON_ONCE(!writable || !refcounted_page))
-		return KVM_PFN_ERR_FAULT;
+kvm_pfn_t gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn)
+{
+	return __gfn_to_pfn_memslot(slot, gfn, false, false, NULL, true,
+				    NULL, NULL);
+}
+EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot);
 
-	*writable = false;
-	*refcounted_page = NULL;
+kvm_pfn_t gfn_to_pfn_memslot_atomic(const struct kvm_memory_slot *slot, gfn_t gfn)
+{
+	return __gfn_to_pfn_memslot(slot, gfn, true, false, NULL, true,
+				    NULL, NULL);
+}
+EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);
 
-	return kvm_follow_pfn(&kfp);
+kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)
+{
+	return gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn);
 }
-EXPORT_SYMBOL_GPL(__kvm_faultin_pfn);
+EXPORT_SYMBOL_GPL(gfn_to_pfn);
 
-int kvm_prefetch_pages(struct kvm_memory_slot *slot, gfn_t gfn,
-		       struct page **pages, int nr_pages)
+int gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,
+			    struct page **pages, int nr_pages)
 {
 	unsigned long addr;
 	gfn_t entry = 0;
@@ -3000,92 +3056,193 @@ int kvm_prefetch_pages(struct kvm_memory
 
 	return get_user_pages_fast_only(addr, nr_pages, FOLL_WRITE, pages);
 }
-EXPORT_SYMBOL_GPL(kvm_prefetch_pages);
+EXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);
 
 /*
- * Don't use this API unless you are absolutely, positively certain that KVM
- * needs to get a struct page, e.g. to pin the page for firmware DMA.
- *
- * FIXME: Users of this API likely need to FOLL_PIN the page, not just elevate
- *	  its refcount.
+ * Do not use this helper unless you are absolutely certain the gfn _must_ be
+ * backed by 'struct page'.  A valid example is if the backing memslot is
+ * controlled by KVM.  Note, if the returned page is valid, it's refcount has
+ * been elevated by gfn_to_pfn().
  */
-struct page *__gfn_to_page(struct kvm *kvm, gfn_t gfn, bool write)
+struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)
 {
-	struct page *refcounted_page = NULL;
-	struct kvm_follow_pfn kfp = {
-		.slot = gfn_to_memslot(kvm, gfn),
-		.gfn = gfn,
-		.flags = write ? FOLL_WRITE : 0,
-		.refcounted_page = &refcounted_page,
-	};
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = gfn_to_pfn(kvm, gfn);
+
+	if (is_error_noslot_pfn(pfn))
+		return KVM_ERR_PTR_BAD_PAGE;
 
-	(void)kvm_follow_pfn(&kfp);
-	return refcounted_page;
+	page = kvm_pfn_to_refcounted_page(pfn);
+	if (!page)
+		return KVM_ERR_PTR_BAD_PAGE;
+
+	return page;
 }
-EXPORT_SYMBOL_GPL(__gfn_to_page);
+EXPORT_SYMBOL_GPL(gfn_to_page);
 
-int __kvm_vcpu_map(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
-		   bool writable)
+void kvm_release_pfn(kvm_pfn_t pfn, bool dirty)
 {
-	struct kvm_follow_pfn kfp = {
-		.slot = gfn_to_memslot(vcpu->kvm, gfn),
-		.gfn = gfn,
-		.flags = writable ? FOLL_WRITE : 0,
-		.refcounted_page = &map->pinned_page,
-		.pin = true,
-	};
+	if (dirty)
+		kvm_release_pfn_dirty(pfn);
+	else
+		kvm_release_pfn_clean(pfn);
+}
 
-	map->pinned_page = NULL;
-	map->page = NULL;
-	map->hva = NULL;
-	map->gfn = gfn;
-	map->writable = writable;
+int kvm_vcpu_map(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map)
+{
+	kvm_pfn_t pfn;
+	void *hva = NULL;
+	struct page *page = KVM_UNMAPPED_PAGE;
 
-	map->pfn = kvm_follow_pfn(&kfp);
-	if (is_error_noslot_pfn(map->pfn))
+	if (!map)
 		return -EINVAL;
 
-	if (pfn_valid(map->pfn)) {
-		map->page = pfn_to_page(map->pfn);
-		map->hva = kmap(map->page);
+	pfn = gfn_to_pfn(vcpu->kvm, gfn);
+	if (is_error_noslot_pfn(pfn))
+		return -EINVAL;
+
+	if (pfn_valid(pfn)) {
+		page = pfn_to_page(pfn);
+		hva = kmap(page);
 #ifdef CONFIG_HAS_IOMEM
 	} else {
-		map->hva = memremap(pfn_to_hpa(map->pfn), PAGE_SIZE, MEMREMAP_WB);
+		hva = memremap(pfn_to_hpa(pfn), PAGE_SIZE, MEMREMAP_WB);
 #endif
 	}
 
-	return map->hva ? 0 : -EFAULT;
+	if (!hva)
+		return -EFAULT;
+
+	map->page = page;
+	map->hva = hva;
+	map->pfn = pfn;
+	map->gfn = gfn;
+
+	return 0;
 }
-EXPORT_SYMBOL_GPL(__kvm_vcpu_map);
+EXPORT_SYMBOL_GPL(kvm_vcpu_map);
 
-void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map)
+void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty)
 {
+	if (!map)
+		return;
+
 	if (!map->hva)
 		return;
 
-	if (map->page)
+	if (map->page != KVM_UNMAPPED_PAGE)
 		kunmap(map->page);
 #ifdef CONFIG_HAS_IOMEM
 	else
 		memunmap(map->hva);
 #endif
 
-	if (map->writable)
+	if (dirty)
 		kvm_vcpu_mark_page_dirty(vcpu, map->gfn);
 
-	if (map->pinned_page) {
-		if (map->writable)
-			kvm_set_page_dirty(map->pinned_page);
-		kvm_set_page_accessed(map->pinned_page);
-		unpin_user_page(map->pinned_page);
-	}
+	kvm_release_pfn(map->pfn, dirty);
 
 	map->hva = NULL;
 	map->page = NULL;
-	map->pinned_page = NULL;
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_unmap);
 
+static bool kvm_is_ad_tracked_page(struct page *page)
+{
+	/*
+	 * Per page-flags.h, pages tagged PG_reserved "should in general not be
+	 * touched (e.g. set dirty) except by its owner".
+	 */
+	return !PageReserved(page);
+}
+
+static void kvm_set_page_dirty(struct page *page)
+{
+	if (kvm_is_ad_tracked_page(page))
+		SetPageDirty(page);
+}
+
+static void kvm_set_page_accessed(struct page *page)
+{
+	if (kvm_is_ad_tracked_page(page))
+		mark_page_accessed(page);
+}
+
+void kvm_release_page_clean(struct page *page)
+{
+	WARN_ON(is_error_page(page));
+
+	kvm_set_page_accessed(page);
+	put_page(page);
+}
+EXPORT_SYMBOL_GPL(kvm_release_page_clean);
+
+void kvm_release_pfn_clean(kvm_pfn_t pfn)
+{
+	struct page *page;
+
+	if (is_error_noslot_pfn(pfn))
+		return;
+
+	page = kvm_pfn_to_refcounted_page(pfn);
+	if (!page)
+		return;
+
+	kvm_release_page_clean(page);
+}
+EXPORT_SYMBOL_GPL(kvm_release_pfn_clean);
+
+void kvm_release_page_dirty(struct page *page)
+{
+	WARN_ON(is_error_page(page));
+
+	kvm_set_page_dirty(page);
+	kvm_release_page_clean(page);
+}
+EXPORT_SYMBOL_GPL(kvm_release_page_dirty);
+
+void kvm_release_pfn_dirty(kvm_pfn_t pfn)
+{
+	struct page *page;
+
+	if (is_error_noslot_pfn(pfn))
+		return;
+
+	page = kvm_pfn_to_refcounted_page(pfn);
+	if (!page)
+		return;
+
+	kvm_release_page_dirty(page);
+}
+EXPORT_SYMBOL_GPL(kvm_release_pfn_dirty);
+
+/*
+ * Note, checking for an error/noslot pfn is the caller's responsibility when
+ * directly marking a page dirty/accessed.  Unlike the "release" helpers, the
+ * "set" helpers are not to be used when the pfn might point at garbage.
+ */
+void kvm_set_pfn_dirty(kvm_pfn_t pfn)
+{
+	if (WARN_ON(is_error_noslot_pfn(pfn)))
+		return;
+
+	if (pfn_valid(pfn))
+		kvm_set_page_dirty(pfn_to_page(pfn));
+}
+EXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);
+
+void kvm_set_pfn_accessed(kvm_pfn_t pfn)
+{
+	if (WARN_ON(is_error_noslot_pfn(pfn)))
+		return;
+
+	if (pfn_valid(pfn))
+		kvm_set_page_accessed(pfn_to_page(pfn));
+}
+EXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);
+
 static int next_segment(unsigned long len, int offset)
 {
 	if (len > PAGE_SIZE - offset)
@@ -3763,19 +3920,17 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_kick);
 
 int kvm_vcpu_yield_to(struct kvm_vcpu *target)
 {
+	struct pid *pid;
 	struct task_struct *task = NULL;
-	int ret;
-
-	if (!read_trylock(&target->pid_lock))
-		return 0;
-
-	if (target->pid)
-		task = get_pid_task(target->pid, PIDTYPE_PID);
-
-	read_unlock(&target->pid_lock);
+	int ret = 0;
 
+	rcu_read_lock();
+	pid = rcu_dereference(target->pid);
+	if (pid)
+		task = get_pid_task(pid, PIDTYPE_PID);
+	rcu_read_unlock();
 	if (!task)
-		return 0;
+		return ret;
 	ret = yield_to(task, 1);
 	put_task_struct(task);
 
@@ -3864,71 +4019,59 @@ bool __weak kvm_arch_dy_has_pending_inte
 
 void kvm_vcpu_on_spin(struct kvm_vcpu *me, bool yield_to_kernel_mode)
 {
-	int nr_vcpus, start, i, idx, yielded;
 	struct kvm *kvm = me->kvm;
 	struct kvm_vcpu *vcpu;
+	int last_boosted_vcpu;
+	unsigned long i;
+	int yielded = 0;
 	int try = 3;
+	int pass;
 
-	nr_vcpus = atomic_read(&kvm->online_vcpus);
-	if (nr_vcpus < 2)
-		return;
-
-	/* Pairs with the smp_wmb() in kvm_vm_ioctl_create_vcpu(). */
-	smp_rmb();
-
+	last_boosted_vcpu = READ_ONCE(kvm->last_boosted_vcpu);
 	kvm_vcpu_set_in_spin_loop(me, true);
-
 	/*
-	 * The current vCPU ("me") is spinning in kernel mode, i.e. is likely
-	 * waiting for a resource to become available.  Attempt to yield to a
-	 * vCPU that is runnable, but not currently running, e.g. because the
-	 * vCPU was preempted by a higher priority task.  With luck, the vCPU
-	 * that was preempted is holding a lock or some other resource that the
-	 * current vCPU is waiting to acquire, and yielding to the other vCPU
-	 * will allow it to make forward progress and release the lock (or kick
-	 * the spinning vCPU, etc).
-	 *
-	 * Since KVM has no insight into what exactly the guest is doing,
-	 * approximate a round-robin selection by iterating over all vCPUs,
-	 * starting at the last boosted vCPU.  I.e. if N=kvm->last_boosted_vcpu,
-	 * iterate over vCPU[N+1]..vCPU[N-1], wrapping as needed.
-	 *
-	 * Note, this is inherently racy, e.g. if multiple vCPUs are spinning,
-	 * they may all try to yield to the same vCPU(s).  But as above, this
-	 * is all best effort due to KVM's lack of visibility into the guest.
-	 */
-	start = READ_ONCE(kvm->last_boosted_vcpu) + 1;
-	for (i = 0; i < nr_vcpus; i++) {
-		idx = (start + i) % nr_vcpus;
-		if (idx == me->vcpu_idx)
-			continue;
-
-		vcpu = xa_load(&kvm->vcpu_array, idx);
-		if (!READ_ONCE(vcpu->ready))
-			continue;
-		if (kvm_vcpu_is_blocking(vcpu) && !vcpu_dy_runnable(vcpu))
-			continue;
-
-		/*
-		 * Treat the target vCPU as being in-kernel if it has a pending
-		 * interrupt, as the vCPU trying to yield may be spinning
-		 * waiting on IPI delivery, i.e. the target vCPU is in-kernel
-		 * for the purposes of directed yield.
-		 */
-		if (READ_ONCE(vcpu->preempted) && yield_to_kernel_mode &&
-		    !kvm_arch_dy_has_pending_interrupt(vcpu) &&
-		    !kvm_arch_vcpu_preempted_in_kernel(vcpu))
-			continue;
+	 * We boost the priority of a VCPU that is runnable but not
+	 * currently running, because it got preempted by something
+	 * else and called schedule in __vcpu_run.  Hopefully that
+	 * VCPU is holding the lock that we need and will release it.
+	 * We approximate round-robin by starting at the last boosted VCPU.
+	 */
+	for (pass = 0; pass < 2 && !yielded && try; pass++) {
+		kvm_for_each_vcpu(i, vcpu, kvm) {
+			if (!pass && i <= last_boosted_vcpu) {
+				i = last_boosted_vcpu;
+				continue;
+			} else if (pass && i > last_boosted_vcpu)
+				break;
+			if (!READ_ONCE(vcpu->ready))
+				continue;
+			if (vcpu == me)
+				continue;
+			if (kvm_vcpu_is_blocking(vcpu) && !vcpu_dy_runnable(vcpu))
+				continue;
 
-		if (!kvm_vcpu_eligible_for_directed_yield(vcpu))
-			continue;
+			/*
+			 * Treat the target vCPU as being in-kernel if it has a
+			 * pending interrupt, as the vCPU trying to yield may
+			 * be spinning waiting on IPI delivery, i.e. the target
+			 * vCPU is in-kernel for the purposes of directed yield.
+			 */
+			if (READ_ONCE(vcpu->preempted) && yield_to_kernel_mode &&
+			    !kvm_arch_dy_has_pending_interrupt(vcpu) &&
+			    !kvm_arch_vcpu_preempted_in_kernel(vcpu))
+				continue;
+			if (!kvm_vcpu_eligible_for_directed_yield(vcpu))
+				continue;
 
-		yielded = kvm_vcpu_yield_to(vcpu);
-		if (yielded > 0) {
-			WRITE_ONCE(kvm->last_boosted_vcpu, i);
-			break;
-		} else if (yielded < 0 && !--try) {
-			break;
+			yielded = kvm_vcpu_yield_to(vcpu);
+			if (yielded > 0) {
+				WRITE_ONCE(kvm->last_boosted_vcpu, i);
+				break;
+			} else if (yielded < 0) {
+				try--;
+				if (!try)
+					break;
+			}
 		}
 	}
 	kvm_vcpu_set_in_spin_loop(me, false);
@@ -4025,9 +4168,9 @@ static int vcpu_get_pid(void *data, u64
 {
 	struct kvm_vcpu *vcpu = data;
 
-	read_lock(&vcpu->pid_lock);
-	*val = pid_nr(vcpu->pid);
-	read_unlock(&vcpu->pid_lock);
+	rcu_read_lock();
+	*val = pid_nr(rcu_dereference(vcpu->pid));
+	rcu_read_unlock();
 	return 0;
 }
 
@@ -4313,14 +4456,7 @@ static long kvm_vcpu_ioctl(struct file *
 		r = -EINVAL;
 		if (arg)
 			goto out;
-
-		/*
-		 * Note, vcpu->pid is primarily protected by vcpu->mutex. The
-		 * dedicated r/w lock allows other tasks, e.g. other vCPUs, to
-		 * read vcpu->pid while this vCPU is in KVM_RUN, e.g. to yield
-		 * directly to this vCPU
-		 */
-		oldpid = vcpu->pid;
+		oldpid = rcu_access_pointer(vcpu->pid);
 		if (unlikely(oldpid != task_pid(current))) {
 			/* The thread running this VCPU changed. */
 			struct pid *newpid;
@@ -4330,10 +4466,9 @@ static long kvm_vcpu_ioctl(struct file *
 				break;
 
 			newpid = get_task_pid(current, PIDTYPE_PID);
-			write_lock(&vcpu->pid_lock);
-			vcpu->pid = newpid;
-			write_unlock(&vcpu->pid_lock);
-
+			rcu_assign_pointer(vcpu->pid, newpid);
+			if (oldpid)
+				synchronize_rcu();
 			put_pid(oldpid);
 		}
 		vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
diff -rupN a/virt/kvm/kvm_mm.h b/virt/kvm/kvm_mm.h
--- a/virt/kvm/kvm_mm.h	2024-12-22 22:22:21.000000000 +0100
+++ b/virt/kvm/kvm_mm.h	2024-12-28 12:18:30.585119978 +0100
@@ -53,7 +53,8 @@ struct kvm_follow_pfn {
 	struct page **refcounted_page;
 };
 
-kvm_pfn_t hva_to_pfn(struct kvm_follow_pfn *kfp);
+kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool interruptible,
+		     bool *async, bool write_fault, bool *writable);
 
 #ifdef CONFIG_HAVE_KVM_PFNCACHE
 void gfn_to_pfn_cache_invalidate_start(struct kvm *kvm,
